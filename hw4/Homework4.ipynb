{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "### Minjie Fan, 998585352\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $1/d$, we first define $f(x) = 1/x-d$, and correspondingly $f'(x)=-x^{-2}$. In this way, we have transformed the problem of computing the reciprocal of $d$ to finding the root of $f(x)$. The iteration formula of Newton's method is\n",
    "$$x_{t+1}=x_t-\\frac{f(x_t)}{f'(x_t)}=x_t+f(x_t)x_t^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x, d) = 1.0/x-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_find (generic function with 3 methods)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function root_find( d, f, x0, optTol = 1e-6, max_iter = 1e4 )\n",
    "    x = NaN\n",
    "    t = NaN\n",
    "    for t = 1:max_iter\n",
    "        x = x0+f(x0, d)*x0^2\n",
    "        if abs(x-x0)<optTol\n",
    "            break\n",
    "        end\n",
    "        x0 = x\n",
    "    end\n",
    "    return (x, t)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36787944117077825,4.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starts from 0.3\n",
    "root_find( exp(1), f, 0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-Inf,10000.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starts from 1.0\n",
    "root_find( exp(1), f, 1.0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we can see that when the initial value is 0.3, the algorithm converges very quickly to the true value (in four steps in our case). On the contrary, if the initial value is not sufficiently close to the true value (say 1.0), then the algorithm may not converge at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By $f(x)=x^q, q>2$, we have\n",
    "$$f'(x)=qx^{q-1}.$$\n",
    "\n",
    "Newton's method gives\n",
    "$$x_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}.$$\n",
    "This implies \n",
    "$$\\lim_{k \\rightarrow \\infty}\\frac{\\lvert x_{k+1} \\rvert}{\\lvert x_k \\rvert}=\\frac{q-1}{q}\\in (0,1),$$\n",
    "i.e., Newton's method converges Q-linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By $f(x)=\\lVert x \\rVert_2^\\beta$, $\\beta>1$, we have\n",
    "$$\\nabla f(x)=\\beta \\lVert x \\rVert_2^{\\beta-2}x,$$\n",
    "and\n",
    "$$\\nabla^2 f(x)=\n",
    "\\begin{cases}\n",
    "\\beta I \\quad \\mbox{if } \\beta=2\\\\\n",
    "\\beta(\\beta-2)\\lVert x \\rVert_2^{\\beta-4}(\\frac{1}{\\beta-2} \\lVert x \\rVert_2^2 I+xx') \\quad \\mbox{if } \\beta \\neq 2.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Sherman-Morrison-Woodbury formula, we have\n",
    "$$(\\nabla^2 f(x))^{-1}=\n",
    "\\begin{cases}\n",
    "I/\\beta \\quad \\mbox{if } \\beta=2\\\\\n",
    "\\frac{1}{\\beta\\lVert x \\rVert_2^{\\beta-2}}\\left( I-\\frac{(\\beta-2)xx'}{(\\beta-1)\\lVert x \\rVert_2^2} \\right) \\quad \\mbox{if } \\beta \\neq 2.\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the pure form of Newton's method is\n",
    "$$x_{k+1}=x_k-(\\nabla^2 f(x_k))^{-1}\\nabla f(x_k)=\n",
    "\\begin{cases}\n",
    "0 \\quad \\mbox{if } \\beta=2\\\\\n",
    "(\\beta-2)/(\\beta-1) x_k \\quad \\mbox{if } \\beta \\neq 2\n",
    "\\end{cases}.$$\n",
    "Thus, \n",
    "$$x_k=\\frac{(\\beta-2)^k}{(\\beta-1)^k}x_0 \\quad \\mbox{if } \\beta \\neq 2.$$\n",
    "\n",
    "* If $1<\\beta<3/2$, we have $\\lvert (\\beta-2)/(\\beta-1)\\rvert>1$. In this case, the algorithm does not converge except $x_0=0$. When $x_0=0$, the algorithm converges to the optimal solution automatically.\n",
    "\n",
    "* If $\\beta=2$, the algorithm converges to the optimal solution in one step.\n",
    "\n",
    "* If $\\beta=3/2$, we have $x_k=(-1)^k x_0$. In this case, the algorithm does not converge except $x_0=0$. When $x_0=0$, the algorithm converges to the optimal solution automatically.\n",
    "\n",
    "* If $\\beta>3/2$ and $\\beta \\neq 2$, we have $0<\\lvert (\\beta-2)/(\\beta-1)\\rvert<1$. Given\n",
    "$$\\lim_{k\\rightarrow \\infty}\\frac{\\lvert x_{k+1} \\rvert}{\\lvert x_k \\rvert}=\\left\\lvert \\frac{\\beta-2}{\\beta-1} \\right\\rvert \\in (0,1),$$\n",
    "the algorithm converges Q-linearly to the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\beta=1$, $$\\nabla f(x)=\\lVert x \\rVert_2^{-1}x,$$\n",
    "and\n",
    "$$\\nabla^2 f(x)=-\\lVert x \\rVert_2^{-3}(xx'-\\lVert x \\rVert_2^2I)=-\\lVert x \\rVert_2^{-1}(\\tilde{x}\\tilde{x}'-I),$$\n",
    "where $\\tilde{x}=x/\\lVert x \\rVert_2$.\n",
    "\n",
    "It is clear that the matrix $\\tilde{x}\\tilde{x}'-I$ is singular. Thus, Newton's method does not work in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\beta<1$, similarly as in the case of $\\beta>1$, we have\n",
    "$$x_{k+1}=(\\beta-2)/(\\beta-1) x_k.$$\n",
    "Then $$\\left\\lvert \\frac{\\beta-2}{\\beta-1} \\right\\rvert=\\frac{2-\\beta}{1-\\beta}>1.$$\n",
    "In this case, the algorithm does not converge except $x_0=0$. When $x_0=0$, the algorithm converges to the optimal solution automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton's method with an Armijo linesearch is\n",
    "$$x_{k+1}=\n",
    "x_k-\\alpha_k \\frac{1}{\\beta-1}x_k=\\left( 1-\\alpha_k \\frac{1}{\\beta-1} \\right)x_k \\quad \\mbox{if } \\beta \\neq 1.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Armijo linesearch, we find $\\alpha_k$ such that\n",
    "$$f(x_k+\\alpha_k p_k)\\leq f(x_k)+c_1 \\alpha_k \\nabla^Tf(x_k)p_k$$\n",
    "Plug in the expressions of $p_k, \\nabla f(x_k)$, we have\n",
    "$$\\left( \\left\\lvert 1-\\frac{\\alpha_k}{\\beta-1} \\right\\rvert^\\beta-1 \\right)\\lVert x_k \\rVert_2^\\beta\\leq c_1 \\alpha_k \\frac{\\beta}{1-\\beta}\\lVert x_k \\rVert_2^\\beta.$$\n",
    "Thus, as long as the algorithm has not reached zero, the Armijo linesearch finds the same $\\alpha_k$ all the time, i.e., $\\alpha_k \\equiv \\alpha$, where $\\alpha$ satisfies\n",
    "$$ \\left\\lvert 1-\\frac{\\alpha}{\\beta-1} \\right\\rvert^\\beta \\leq c_1 \\alpha \\frac{\\beta}{1-\\beta}+1.$$\n",
    "Thus, each update becomes \n",
    "$$x_{k+1}=\\left( 1- \\frac{\\alpha}{\\beta-1} \\right)x_k \\quad \\mbox{if } \\beta \\neq 1,$$\n",
    "and \n",
    "$$x_k=\\left( 1- \\frac{\\alpha}{\\beta-1} \\right)^k x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have\n",
    "$$\\left\\lvert 1- \\frac{\\alpha}{\\beta-1} \\right\\rvert\\leq \\left( 1-c_1 \\alpha \\frac{\\beta}{\\beta-1} \\right)^{1/\\beta}.$$\n",
    "When $\\beta>1$, $$1-c_1 \\alpha \\frac{\\beta}{\\beta-1}<1,$$\n",
    "which implies the algorithm converges to the optimal solution.\n",
    "\n",
    "When $\\beta<1$, \n",
    "$$ 1- \\frac{\\alpha}{\\beta-1}=1+\\frac{\\alpha}{1-\\beta}>1,$$\n",
    "which implies the algorithm does not converge except $x_0=0$. When $x_0=0$, the algorithm converges to the optimal solution automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\beta=1$, the Hessian matrix is singular. Thus, Newton's method does not work in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use Mathematical induction to show $D^{k+1}q^i=p^i$.\n",
    "\n",
    "* When $k=0$, $i=0$ as well. We need to show $D^1q^0=p^0$. We starts from the LHS.\n",
    "$$D^1 q^0=\\left(D^0+\\frac{y^0y^{0'}}{q^{0'}y^0}\\right)q^0=D^0q^0+\\frac{y^0(y^{0'}q^0)}{q^{0'}y^0}=D^0q^0+y^0=p^0.$$\n",
    "\n",
    "* Suppose $k-1$ holds. To show $D^{k+1}q^i=p^i$, for all $i\\leq k$. We starts from the LHS.\n",
    "$$D^{k+1}q^i=\\left( D^k+\\frac{y^ky^{k'}}{q^{k'}y^k} \\right)q^i.$$\n",
    "\n",
    "If $i<k$, then \n",
    "$$LHS=p^i+\\frac{1}{q^{k'}y^k}y^k(y^{k'}q^i)=p^i+\\frac{1}{q^{k'}y^k}y^k(p^{k'}q^i-q^{k'}D^kq^i)＝p^i+\\frac{1}{q^{k'}y^k}y^k(p^{k'}q^i-q^{k'}p^i).$$\n",
    "\n",
    "Since the objective function is positive definite quadratic, i.e., $f(x)=1/2x'Qx-b'x$, where $Q>0$, we have\n",
    "$q^i = Qp^i$. \n",
    "\n",
    "Then $p^{k'}q^i-q^{k'}p^i=0$, and thus $LHS=p^i=RHS$.\n",
    "\n",
    "If $i=k$, then\n",
    "$$LHS=D^kq^k+\\frac{y^k(y^{k'}q^k)}{q^{k'}y^k}=p^k-y^k+\\frac{y^k(y^{k'}q^k)}{q^{k'}y^k}=p^k=RHS.$$\n",
    "\n",
    "Thus, $D^{k+1}q^i=p^i$ has been shown.\n",
    "\n",
    "* Since $D^{k+1}q^i=p^i$ and $q^i = Qp^i$, we have\n",
    "$D^{n}q^i=Q^{-1}q^i$, for all $i\\leq n-1$. The assumption that $q^0,\\cdots, q^{n-1}$ are linearly independent implies the solution space of $(D^n-Q^{-1})x=0$ is of dimension $n$. Thus, $rank(D^n-Q^{-1})=0$, i.e. $D^n=Q^{-1}$, where $Q$ is just the Hessian of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Toms566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Backtracking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backtrack (generic function with 4 methods)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backtrack( obj, grd, x, d, rho=0.9, alpha=1.0, c=1e-4 )\n",
    "    # Find step size by backtracking\n",
    "    gxp = (grd(x)'*d)[1]\n",
    "    while obj(x+alpha*d)>obj(x)+c*alpha*gxp\n",
    "        alpha *= rho\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Function of Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 4 methods)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the template provided\n",
    "function newtmin( obj, grd, hes, x, maxIts=100, optTol=1e-6, modify=1 )\n",
    "    # the sqrt of the machine precision\n",
    "    delta = 1e-8\n",
    "    n = length(x)\n",
    "    t = NaN\n",
    "    d = NaN\n",
    "    for t = 1:maxIts\n",
    "        f = obj(x)\n",
    "        g = grd(x)\n",
    "        H = hes(x)\n",
    "        # check condition for breaking\n",
    "        if norm(g)<optTol\n",
    "            break\n",
    "        end\n",
    "        # try chol decomp first\n",
    "        try\n",
    "            F = chol(H)\n",
    "            d = -R\\(R'\\g)\n",
    "        # o.w., use eigenvalue modification\n",
    "        catch\n",
    "            F = eigfact(H)\n",
    "            # method 1\n",
    "            if modify==1\n",
    "                Lambda_inv = 1./max(abs(F[:values]), delta)\n",
    "            # method 2\n",
    "            else\n",
    "                Lambda_inv = 1./max(F[:values], delta)\n",
    "            end\n",
    "            d = -F[:vectors]*Diagonal(Lambda_inv)*F[:vectors]'*g\n",
    "        end\n",
    "        # find alpha\n",
    "        alpha = backtrack( obj, grd, x, d )\n",
    "        # update x\n",
    "        x = x+alpha*d\n",
    "    end\n",
    "    return (x, t-1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Bisection Method for the Wolfe Conditions Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last homework, we have explored the implementation of BFGS method. It shows that the linesearch by backtracking does not work very well since the curvature condition is not ensured. To alleviate this issue, we use the linesearch by bisection instead. It is possible that we can not find an $\\alpha$ such that the Wolfe conditions are satisfied. In this case, we will just exit the loop as the number of iterations exceeds the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bisection (generic function with 8 methods)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bisection( obj, grd, x, d, rho=0.9, alpha=1.0, alpha_l=0.0, alpha_r=100.0, c1=1e-4, c2=0.9, max_iter=100 )\n",
    "    # Find step size by bisection method\n",
    "    for i = 1:max_iter\n",
    "        gxp = (grd(x)'*d)[1]\n",
    "        x_new = x+alpha*d\n",
    "        gxp_new = (grd(x_new)'*d)[1]\n",
    "        if obj(x_new)>obj(x)+c1*alpha*gxp\n",
    "            alpha_r = alpha\n",
    "            alpha = (alpha_l+alpha_r)/2.0\n",
    "        elseif gxp_new<c2*gxp\n",
    "            alpha_l = alpha\n",
    "            if alpha_r>=100.0\n",
    "                alpha = 2*alpha_l\n",
    "            else\n",
    "                alpha = (alpha_l+alpha_r)/2.0\n",
    "            end\n",
    "        else \n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Function of Quasi-Newton's Method (BFGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtminBFGS (generic function with 3 methods)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the template provided\n",
    "function newtminBFGS( obj, grd, hes, x, maxIts=100, optTol=1e-6 )\n",
    "    # Minimize a function f using Newton’s method.\n",
    "    n = length(x)\n",
    "    # the init value of H\n",
    "    H_approx = eye(n)\n",
    "    t = NaN\n",
    "    f_pre = NaN\n",
    "    for t = 1:maxIts\n",
    "        f = obj(x)\n",
    "        g = grd(x)\n",
    "        H = hes(x)\n",
    "        if norm(g)<optTol\n",
    "            break\n",
    "        end\n",
    "        # get dir\n",
    "        d = -H_approx*g\n",
    "        # find alpha\n",
    "        alpha = bisection( obj, grd, x, d )\n",
    "        # update x\n",
    "        x_new = x+alpha*d\n",
    "        # get s\n",
    "        s = x_new-x\n",
    "        # get y\n",
    "        y = grd(x_new)-g\n",
    "        rho_k = 1/(y'*s)[1]\n",
    "        # update H\n",
    "        H_approx = (eye(n)-rho_k*s*y')*H_approx*(eye(n)-rho_k*y*s')+rho_k*s*s'\n",
    "        \n",
    "        # update x and f_pre\n",
    "        x = x_new\n",
    "        f_pre = f\n",
    "    end\n",
    "    return (x, t-1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_Newton_BFGS (generic function with 1 method)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_Newton_BFGS()\n",
    "    @printf(\"%3s %12s %12s %6s %12s %12s %6s\\n\",\n",
    "    \"No.\", \"Newton f(x*)\", \"|∇f(x*)|\", \"Iters\", \"BFGS f(x*)\", \"|∇f(x*)|\", \"Iters\")\n",
    "    modify = ones(18)\n",
    "    modify[1] = 2\n",
    "    for i = 1:18\n",
    "        p = Problem(i);\n",
    "        x = p.x0\n",
    "        res_newton = newtmin( p.obj, p.grd, p.hes, x, 1e3, 1e-8, modify[i] )\n",
    "        res_BFGS = newtminBFGS( p.obj, p.grd, p.hes, x, 1e3, 1e-8 )\n",
    "        x_star_newton = res_newton[1]\n",
    "        iter_newton = res_newton[2]\n",
    "        x_star_BFGS = res_BFGS[1]\n",
    "        iter_BFGS = res_BFGS[2]\n",
    "        @printf(\"%3s %12e %12e %6i %12e %12e %6i\\n\",\n",
    "        i, p.obj(x_star_newton), norm(p.grd(x_star_newton)), iter_newton, p.obj(x_star_BFGS), norm(p.grd(x_star_BFGS)), iter_BFGS)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. Newton f(x*)     |∇f(x*)|  Iters   BFGS f(x*)     |∇f(x*)|  Iters\n",
      "  1 7.141922e-34 1.770799e-16     14 1.123145e-20 2.879516e-09     32\n",
      "  2 1.669195e-03 3.070969e-03    999 5.655650e-03 5.753761e-09     37\n",
      "  3 1.127933e-08 9.701954e-11      2 1.127933e-08 3.980232e-11      5\n",
      "  4 7.694100e-31 1.754301e-15     85 4.942521e-32 4.043944e-11    155\n",
      "  5 1.307394e-21 2.709262e-11     26 6.549517e-19 2.881106e-09     31\n",
      "  6 1.621755e+01 2.059218e+01    999 1.987998e-19 1.087393e-09     31\n",
      "  7 4.320501e-04 2.807938e-02    999 1.399760e-06 3.318211e-09     70\n",
      "  8 5.250351e-04 8.434708e-11     20 5.250351e-04 3.844472e-09    199\n",
      "  9 8.803193e+01 6.736629e-03    999 8.803186e+01 9.752865e-09    524\n",
      " 10 1.262177e-29 7.105427e-09      8 0.000000e+00 0.000000e+00     12\n",
      " 11 8.582220e+04 2.220736e-08    999 8.582220e+04 4.524572e-10     31\n",
      " 12 5.742068e-29 8.715036e-14     14 3.628054e-17 4.551833e-09     32\n",
      " 13 5.068991e-17 9.961953e-09    386 3.946270e-06 8.825772e-09     56\n",
      " 14 5.963014e+01 1.283421e+01    999 4.432613e-17 8.926977e-09    248\n",
      " 15 4.610843e-03 8.055350e-02    999 2.534395e-12 9.852833e-09    202\n",
      " 16 2.959860e-19 9.647698e-10      8 6.087295e-18 5.157395e-09     15\n",
      " 17 1.778157e-24 1.076259e-11     38 2.091837e-19 7.661694e-09     28\n",
      " 18 6.649032e-03 3.010350e-02    999 5.386315e-03 6.723224e-09    203\n"
     ]
    }
   ],
   "source": [
    "test_Newton_BFGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we can see that the Newton's method (with eigenvalue modification) converges successfully for problems 1, 3, 4, 5, 8, 10, 12, 13, 16, 17. In contrast, the BFGS method converges for all the problems. Generally speaking, the Newton's method converges faster than the BFGS. But the BFGS method needs neither evaluating the Hessian matrix nor solving a linear system of equations. Thus, for each iteration, it is much faster than the Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
