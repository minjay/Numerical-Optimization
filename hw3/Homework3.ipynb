{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "## Minjie Fan, 998585352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only for version 0.4.0\n",
    "using Toms566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Backtracking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backtrack (generic function with 4 methods)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backtrack( obj, grd, x, d, rho=0.9, alpha=1.0, c=1e-4 )\n",
    "    # Find step size by backtracking\n",
    "    gxp = (grd(x)'*d)[1]\n",
    "    while obj(x+alpha*d)>obj(x)+c*alpha*gxp\n",
    "        alpha *= rho\n",
    "    end\n",
    "    return alpha\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Function of Newton's Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 4 methods)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the template provided\n",
    "function newtmin( obj, grd, hes, x, maxIts=100, optTol=1e-6, modify=1 )\n",
    "    # the sqrt of the machine precision\n",
    "    delta = 1e-8\n",
    "    n = length(x)\n",
    "    t = NaN\n",
    "    d = NaN\n",
    "    for t = 1:maxIts\n",
    "        f = obj(x)\n",
    "        g = grd(x)\n",
    "        H = hes(x)\n",
    "        # check condition for breaking\n",
    "        if norm(g)<optTol\n",
    "            break\n",
    "        end\n",
    "        # try chol decomp first\n",
    "        try\n",
    "            F = chol(H)\n",
    "            d = -R\\(R'\\g)\n",
    "        # o.w., use eigenvalue modification\n",
    "        catch\n",
    "            F = eigfact(H)\n",
    "            # method 1\n",
    "            if modify==1\n",
    "                Lambda_inv = 1./max(abs(F[:values]), delta)\n",
    "            # method 2\n",
    "            else\n",
    "                Lambda_inv = 1./max(F[:values], delta)\n",
    "            end\n",
    "            d = -F[:vectors]*Diagonal(Lambda_inv)*F[:vectors]'*g\n",
    "        end\n",
    "        # find alpha\n",
    "        alpha = backtrack( obj, grd, x, d )\n",
    "        # update x\n",
    "        x = x+alpha*d\n",
    "    end\n",
    "    return (x, t-1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Function of Quasi-Newton's Method (BFGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtminBFGS (generic function with 4 methods)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the template provided\n",
    "function newtminBFGS( obj, grd, hes, x, maxIts=100, optTol=1e-6 )\n",
    "    # Minimize a function f using Newton’s method.\n",
    "    n = length(x)\n",
    "    # the init value of H\n",
    "    H_approx = eye(n)\n",
    "    t = NaN\n",
    "    f_pre = NaN\n",
    "    for t = 1:maxIts\n",
    "        f = obj(x)\n",
    "        g = grd(x)\n",
    "        H = hes(x)\n",
    "        # check two conditions for breaking\n",
    "        if norm(g)<optTol || (t>1 && abs((f_pre-f)/f_pre)<optTol)\n",
    "            break\n",
    "        end\n",
    "        # get dir\n",
    "        d = -H_approx*g\n",
    "        # find alpha\n",
    "        alpha = backtrack( obj, grd, x, d )\n",
    "        # update x\n",
    "        x_new = x+alpha*d\n",
    "        # get s\n",
    "        s = x_new-x\n",
    "        # get y\n",
    "        y = grd(x_new)-g\n",
    "        rho_k = 1/(y'*s)[1]\n",
    "        # update H\n",
    "        H_approx = (eye(n)-rho_k*s*y')*H_approx*(eye(n)-rho_k*y*s')+rho_k*s*s'\n",
    "        \n",
    "        # update x and f_pre\n",
    "        x = x_new\n",
    "        f_pre = f\n",
    "    end\n",
    "    return (x, t-1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myhes (generic function with 1 method)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myobj(x) = (x^2)[1]\n",
    "mygrd(x) = 2.0*x\n",
    "myhes(x) = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "1x1 Array{Float64,2}:\n",
       " 0.0,\n",
       "\n",
       "1)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = newtmin( myobj, mygrd, myhes, 10.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "1x1 Array{Float64,2}:\n",
       " 0.0,\n",
       "\n",
       "2)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = newtminBFGS( myobj, mygrd, myhes, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this naive example, the Netwon's method converges to the true minimum in one step, while the Quasi-Newton's method (BFGS) converges to the true minimum in two steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test My Newton Function using Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_Newton (generic function with 1 method)"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_Newton()\n",
    "    @printf(\"%3s %12s %12s %6s %6s %10s\\n\",\n",
    "    \"No.\", \"f(x*)\", \"|∇f(x*)|\", \"Modify\", \"Iters\", \"Converged\")\n",
    "    modify = ones(18)\n",
    "    modify[1] = 2\n",
    "    for i = 1:18\n",
    "        p = Problem(i);\n",
    "        x = p.x0\n",
    "        res = newtmin( p.obj, p.grd, p.hes, x, 1e3, 1e-8, modify[i] )\n",
    "        x_star = res[1]\n",
    "        norm_grd = norm(p.grd(x_star))\n",
    "        if norm_grd<1e-8\n",
    "            conv = \"true\"\n",
    "        else\n",
    "            conv = \"false\"\n",
    "        end\n",
    "        @printf(\"%3s %12e %12e %6i %6i %10s\\n\",\n",
    "        i, p.obj(x_star), norm_grd, modify[i], res[2], conv)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.        f(x*)     |∇f(x*)| Modify  Iters  Converged\n",
      "  1 7.141922e-34 1.770799e-16      2     14       true\n",
      "  2 1.669195e-03 3.070969e-03      1    999      false\n",
      "  3 1.127933e-08 9.701954e-11      1      2       true\n",
      "  4 7.694100e-31 1.754301e-15      1     85       true\n",
      "  5 1.307394e-21 2.709262e-11      1     26       true\n",
      "  6 1.621755e+01 2.059218e+01      1    999      false\n",
      "  7 4.320501e-04 2.807938e-02      1    999      false\n",
      "  8 5.250351e-04 8.434708e-11      1     20       true\n",
      "  9 8.803193e+01 6.736629e-03      1    999      false\n",
      " 10 1.262177e-29 7.105427e-09      1      8       true\n",
      " 11 8.582220e+04 2.220736e-08      1    999      false\n",
      " 12 5.742068e-29 8.715036e-14      1     14       true\n",
      " 13 5.068991e-17 9.961953e-09      1    386       true\n",
      " 14 5.963014e+01 1.283421e+01      1    999      false\n",
      " 15 4.610843e-03 8.055350e-02      1    999      false\n",
      " 16 2.959860e-19 9.647698e-10      1      8       true\n",
      " 17 1.778157e-24 1.076259e-11      1     38       true\n",
      " 18 6.649032e-03 3.010350e-02      1    999      false\n"
     ]
    }
   ],
   "source": [
    "test_Newton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we can see that the Newton's method (with eigenvalue modification) converges successfully for problems 1, 3, 4, 5, 8, 10, 12, 13, 16, 17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test My BFGS Function using Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_BFGS (generic function with 1 method)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_BFGS()\n",
    "    @printf(\"%3s %12s %12s %6s %6s %10s\\n\",\n",
    "    \"No.\", \"f(x*)\", \"Benchmark\", \"|∇f(x*)|\", \"Iters\", \"Converged\")\n",
    "    for i = 1:18\n",
    "        p = Problem(i);\n",
    "        x = p.x0\n",
    "        res = newtminBFGS( p.obj, p.grd, p.hes, x, 1e3, 1e-8 )\n",
    "        res_bench = optimize( p.obj, p.grd!, p.hes!, x, method = :bfgs )\n",
    "        x_star = res[1]\n",
    "        steps = res[2]\n",
    "        if steps<999\n",
    "            conv = \"true\"\n",
    "        else\n",
    "            conv = \"false\"\n",
    "        end\n",
    "        @printf(\"%3s %12e %12e %6e %6i %10s\\n\",\n",
    "        i, p.obj(x_star), res_bench.f_minimum, norm(p.grd(x_star)), steps, conv)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.        f(x*)    Benchmark |∇f(x*)|  Iters  Converged\n",
      "  1 2.837687e-23 5.943187e-20 1.892953e-10     30       true\n",
      "  2 1.498395e-02 5.655650e-03 1.969714e-01     22       true\n",
      "  3 1.127933e-08 1.127933e-08 4.345071e-11      5       true\n",
      "  4 7.661898e-29 4.780622e-24 1.374931e-09    239       true\n",
      "  5 6.613458e-19 8.242706e-23 2.499883e-09     33       true\n",
      "  6 8.319941e-25 2.233991e-21 2.109242e-11     29       true\n",
      "  7 1.399760e-06 1.399760e-06 3.605054e-10     68       true\n",
      "  8 6.786564e-04 5.250351e-04 7.237335e-04     20       true\n",
      "  9 8.806565e+01 8.803248e+01 1.231225e+00     85       true\n",
      " 10 0.000000e+00 1.972152e-31 0.000000e+00     19       true\n",
      " 11 8.582220e+04 8.582220e+04 3.950739e-02     24       true\n",
      " 12 3.039644e+00 9.688373e-24 8.187503e+00     11       true\n",
      " 13 3.946270e-06 3.946270e-06 8.060639e-08     53       true\n",
      " 14 6.310869e-19 4.163020e-20 7.927464e-09    168       true\n",
      " 15 3.419221e-12 1.570424e-12 9.009860e-09    265       true\n",
      " 16 1.102962e-18 2.877199e-24 4.787514e-09     19       true\n",
      " 17 1.706415e-23 1.588591e-19 1.408542e-10     39       true\n",
      " 18 5.386474e-03 5.386316e-03 1.317305e-04    300       true\n"
     ]
    }
   ],
   "source": [
    "test_BFGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table, the benchmark minimial value of the function is computed by package *Optim*. The difficulty of implementing the BFGS method is the linesearch such that the Wolfe conditions are satisfied. However, the backtracking algorithm can only guarantee the satisficaiton of sufficent decrease but not curvature condition. In this case, *NaN* can happen. In order to alleviate this issue, I added another exit condition, which is \n",
    "$$\\frac{|f(x)-f(x')|}{|f(x)|}< \\epsilon.$$ According to my numerical experiments, it can partly solve the issue, but occasionaly, it may cause premature convergence.\n",
    "\n",
    "Compared with the benchmark, all the problems work well except Prob. 12, which converges prematurely. This indicates the necessity of more sophisticated linesearch algorithms.\n",
    "\n",
    "Since Prob. 12 has been successfully solved by the Newton's method, all the problems can be solved by combining the Newton's and BFGS methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = readtable(\"binary.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get y and X\n",
    "y = dt[:admit];\n",
    "X = dt[[:gre, :gpa]];\n",
    "m = size(X, 2)\n",
    "std_vec = [std(X[i]) for i in 1:m]\n",
    "# do feature rescaling\n",
    "for i = 1:m\n",
    "    X[i] = X[i]/std_vec[i]\n",
    "end\n",
    "n = length(y)\n",
    "y = convert(Array, y);\n",
    "X_full = convert(Matrix, [X ones(n)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the negative log-likelihood\n",
    "function negloglik(a, y, X_full)\n",
    "    value = 0.0;\n",
    "    tmp = X_full*a;\n",
    "    for i = 1:length(y)\n",
    "        value = value-y[i]*tmp[i]+log(1+exp(tmp[i]));\n",
    "    end\n",
    "    return value;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the gradient\n",
    "function negloglik_grad(a, y, X_full)\n",
    "    value = zeros(1, size(X_full, 2))\n",
    "    n = length(y);\n",
    "    tmp = exp(X_full*a);\n",
    "    tmp = tmp./(1.+tmp).-y;\n",
    "    for i = 1:n\n",
    "        value = value+tmp[i]*X_full[i, :]\n",
    "    end\n",
    "    return value';\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the Hessian matrix\n",
    "function negloglik_Hess(a, y, X_full)\n",
    "    m = size(X_full, 2);\n",
    "    n = length(y);\n",
    "    tmp = exp(X_full*a);\n",
    "    tmp = tmp./(1.+tmp).^2;\n",
    "    value = zeros(m, m);\n",
    "    for j = 1:m\n",
    "        for k = j:m\n",
    "            for i = 1:n\n",
    "                value[j, k] = value[j, k]+tmp[i]*X_full[i, j]*X_full[i, k];\n",
    "            end\n",
    "            if k>j\n",
    "                value[k, j] = value[j, k];\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return value;\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj(a) = negloglik(a, y, X_full);\n",
    "grd(a) = negloglik_grad(a, y, X_full);\n",
    "hes(a) = negloglik_Hess(a, y, X_full);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "3x1 Array{Float64,2}:\n",
       "  0.310818\n",
       "  0.287209\n",
       " -4.94938 ,\n",
       "\n",
       "32.0)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0 = [1;1;1]\n",
    "res = newtmin( obj, grd, hes, a0, 1e3, 1e-8, 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous homework, the gradient descent converges at the 107425-th iteration, while the modified Newton's method converges at the 32-th iteration. \n",
    "\n",
    "Generally speaking, among the gradient descent, Newton's and BFGS methods, the Newton's converges the fastest, then the BFGS and the gradient descent. But in some cases, the Newton's method does not work very well (due to the ill-conditioned Hessian matrix), and it takes longer time to solve for the descent direction. Comparatively speaking, BFGS is more robust in this sense. The gradient descent can be used when the Hessian matrix is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3x1 Array{Any,2}:\n",
       "  0.00269068\n",
       "  0.754687  \n",
       " -4.94938   "
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = res[1]./[std_vec; 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
